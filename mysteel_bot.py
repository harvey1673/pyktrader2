# -*- coding: gbk -*-
import openpyxl
from openpyxl.worksheet import *
import pickle
from bs4 import BeautifulSoup
import time
from datetime import date, timedelta, datetime
import requests 
#from requests_ntlm import HttpNtlmAuth
import shutil
import os
import re
import copy
import logging
import urllib2
import codecs
import warnings
import numpy as np
import pandas as pd
#from requests_negotiate_sspi import HttpNegotiateAuth
from openpyxl.workbook import Workbook
from openpyxl import load_workbook
from openpyxl.utils import get_column_letter, range_boundaries
from openpyxl.cell import Cell
import lxml
from Crypto.Cipher import AES
from mysteel_config import *
import base
import sys
if sys.version_info[0] < 3:
    from StringIO import StringIO
else:
    from io import StringIO


import subprocess as sp #for running vbscript
from dateutil.relativedelta import relativedelta #for adding dates

from urlparse import urlparse
import base64

proxyDict = {
    "http"  : "http://e372854:Cargill2015@shan-prox-mwg2.ap.corp.cargill.com:4200",
     "https" : "http://e372854:Cargill2015@sing-prox-mwg1.ap.corp.cargill.com:4200",
}

#chrome browser header:
headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}

_session_data = {
    "mysteel": {
        "my_username": "xxxxxx",
        "my_password": "xxxxxx",
        "site": "mysteel",
        "callback": "http://jiancai.mysteel.com/m/16/0729/11/F250E9B9835EC176.html",
        "login_url": "http://passport.mysteel.com/login.jsp"
        },
}

def setup_ntlm():
    '''
    Run this function to read the ntlm_auth.p generated by the script generate_ntlm_auth.py
    '''
    logger = logging.getLogger(__name__)
    try:
        global _session_data
        from os.path import abspath
        file_path =  abspath('{}\\ntlm_auth.p'.format(os.path.dirname(__file__)))
        ntlm_auth  = (pickle.load(open( file_path, "rb" )))
    
        ntlm_auth_dic = {
                        "username"  : ntlm_auth["username"],
                        "password" : _decrypt(ntlm_auth['password']),
                }
        _session_data['ntlm'] = ntlm_auth_dic
    except:
        logger.warn("[_setup_ntlm] Error reading ntlm_auth.p ")
        raise

        
def setup_proxy():
    logger = logging.getLogger(__name__)
    try:
        global proxyDict
        from os.path import abspath
        file_path =  abspath('{}\\proxy_setting.p'.format(os.path.dirname(__file__)))
        print file_path
        http_proxy  = _decrypt(pickle.load(open( file_path, "rb" )))
        https_proxy = http_proxy
        proxyDict = {
                      "http"  : http_proxy,
                      "https" : https_proxy,
                    }
    except:
        logger.warn("[setup_proxy] Proxy settings not detected. Try generating the file using generate_proxy_link.py")
        raise

def _encrypt(msg,key="78nsGs3tosd83jfs"):
    obj = AES.new(key, AES.MODE_CFB, 'This is an IV456')
    return obj.encrypt(msg)

def _decrypt(encrypted_str,key="78nsGs3tosd83jfs"):
    obj = AES.new(key, AES.MODE_CFB, 'This is an IV456')
    return obj.decrypt(encrypted_str)


def parse_date_arg(argv):
    '''
    Pass in the system argv and return a date object to be used by each task function. Date must be passed in "YYYY-MM-DD" format e.g. 2016-01-31
    If argv is empty, last biz day is returned
    '''
    logger = logging.getLogger(__name__)
    if not argv:
        logger.info("[parse_date_arg] No date agrument present")
        return None
    if len(argv) < 2:
        return get_biz_date(-1)
    else:
        try:
            the_date = datetime.strptime(str(argv[1]).strip(),"%Y-%m-%d").date()
            logger.info("[parse_date_arg] Date argument received. Processing for the date {}".format(the_date))
            return the_date
        except Exception as e:
            logger.warn("[parse_date_arg] Invalid date format {}. Setting date to today {}".format(argv[1],get_biz_date(0)))
            return get_biz_date(-1)
        
def _change_my_steel_page_html(url,pg_num):
    '''
    Fit in a URL like http://list1.mysteel.com/article/p-3571-------------1.html
    and return a new url with the new page number
    '''
    #return re.sub("[0-9]*\.html","2.html",'http://list1.mysteel.com/article/p-3571-------------1.html')
    return re.sub("[0-9]*\.html","{}.html".format(pg_num),url)

def insert_rows(self, row_idx, cnt, above=False, copy_style=True, fill_formulae=True):
    """Inserts new (empty) rows into worksheet at specified row index.

    :param row_idx: Row index specifying where to insert new rows.
    :param cnt: Number of rows to insert.
    :param above: Set True to insert rows above specified row index.
    :param copy_style: Set True if new rows should copy style of immediately above row.
    :param fill_formulae: Set True if new rows should take on formula from immediately above row, filled with references new to rows.

    Usage:

    * insert_rows(2, 10, above=True, copy_style=False)

    """
    CELL_RE  = re.compile("(?P<col>\$?[A-Z]+)(?P<row>\$?\d+)")

    row_idx = row_idx - 1 if above else row_idx

    def replace(m):
        row = m.group('row')
        prefix = "$" if row.find("$") != -1 else ""
        row = int(row.replace("$",""))
        row += cnt if row > row_idx else 0
        return m.group('col') + prefix + str(row)

    # First, we shift all cells down cnt rows...
    old_cells = set()
    old_fas   = set()
    new_cells = dict()
    new_fas   = dict()
    for c in self._cells.values():

        old_coor = c.coordinate

        # Shift all references to anything below row_idx
        if c.data_type == Cell.TYPE_FORMULA:
            c.value = CELL_RE.sub(
                replace,
                c.value
            )
            # Here, we need to properly update the formula references to reflect new row indices
            if old_coor in self.formula_attributes and 'ref' in self.formula_attributes[old_coor]:
                self.formula_attributes[old_coor]['ref'] = CELL_RE.sub(
                    replace,
                    self.formula_attributes[old_coor]['ref']
                )

        # Do the magic to set up our actual shift    
        if c.row > row_idx:
            old_coor = c.coordinate
            old_cells.add((c.row,c.col_idx))
            c.row += cnt
            new_cells[(c.row,c.col_idx)] = c
            if old_coor in self.formula_attributes:
                old_fas.add(old_coor)
                fa = self.formula_attributes[old_coor].copy()
                new_fas[c.coordinate] = fa

    for coor in old_cells:
        del self._cells[coor]
    self._cells.update(new_cells)

    for fa in old_fas:
        del self.formula_attributes[fa]
    self.formula_attributes.update(new_fas)

    # Next, we need to shift all the Row Dimensions below our new rows down by cnt...
    for row in range(len(self.row_dimensions)-1+cnt,row_idx+cnt,-1):
        new_rd = copy.copy(self.row_dimensions[row-cnt])
        new_rd.index = row
        self.row_dimensions[row] = new_rd
        del self.row_dimensions[row-cnt]

    # Now, create our new rows, with all the pretty cells
    row_idx += 1
    for row in range(row_idx,row_idx+cnt):
        # Create a Row Dimension for our new row
        new_rd = copy.copy(self.row_dimensions[row-1])
        new_rd.index = row
        self.row_dimensions[row] = new_rd
        for col in range(1,self.max_column):
            col = get_column_letter(col)
            cell = self['%s%d'%(col,row)]
            cell.value = None
            source = self['%s%d'%(col,row-1)]
            if copy_style:
                cell.number_format = source.number_format
                cell.font      = source.font.copy()
                cell.alignment = source.alignment.copy()
                cell.border    = source.border.copy()
                cell.fill      = source.fill.copy()
            if fill_formulae and source.data_type == Cell.TYPE_FORMULA:
                s_coor = source.coordinate
                if s_coor in self.formula_attributes and 'ref' not in self.formula_attributes[s_coor]:
                    fa = self.formula_attributes[s_coor].copy()
                    self.formula_attributes[cell.coordinate] = fa
                # print("Copying formula from cell %s%d to %s%d"%(col,row-1,col,row))
                cell.value = re.sub(
                    "(\$?[A-Z]{1,3}\$?)%d"%(row - 1),
                    lambda m: m.group(1) + str(row),
                    source.value
                )   
                cell.data_type = Cell.TYPE_FORMULA

    # Check for Merged Cell Ranges that need to be expanded to contain new cells
    for cr_idx, cr in enumerate(self.merged_cell_ranges):
        self.merged_cell_ranges[cr_idx] = CELL_RE.sub(
            replace,
            cr
        )

Worksheet.insert_rows = insert_rows

def insert_row(ws, row_idx, cnt, above=False, copy_style=True, fill_formulae=True):
    '''
    Summary: Function insert row at specified row index.
            https://bitbucket.org/snippets/openpyxl/qyzKn/sample-code-for-inserting-rows-in-an
            Usage:
            * insert_rows(2, 10, above=True, copy_style=False)

    Author: Jax
    Date created: 14 Jul 2016
    Date last updated: 14 Jul 2016
    Precondition:
        Required:
                :param row_idx: Row index specifying where to insert new rows.
                :param cnt: Number of rows to insert.
                :param above: Set True to insert rows above specified row index.
                :param copy_style: Set True if new rows should copy style of immediately above row.
                :param fill_formulae: Set True if new rows should take on formula from immediately above row, filled with references new to rows.
        Optional:
            t: t is an integer mark by day. It will return today's date + t. t can be negative for previous day.
    Postcondition:
        Required: N/A
        Optional: N/A
    '''

    CELL_RE  = re.compile("(?P<col>\$?[A-Z]+)(?P<row>\$?\d+)")

    row_idx = row_idx - 1 if above else row_idx

    def replace(m):
        row = m.group('row')
        prefix = "$" if row.find("$") != -1 else ""
        row = int(row.replace("$",""))
        row += cnt if row > row_idx else 0
        return m.group('col') + prefix + str(row)

    # First, we shift all cells down cnt rows...
    old_cells = set()
    old_fas   = set()
    new_cells = dict()
    new_fas   = dict()
    for c in ws._cells.values():

        old_coor = c.coordinate

        # Shift all references to anything below row_idx
        if c.data_type == Cell.TYPE_FORMULA:
            c.value = CELL_RE.sub(
                replace,
                c.value
            )
            # Here, we need to properly update the formula references to reflect new row indices
            if old_coor in ws.formula_attributes and 'ref' in ws.formula_attributes[old_coor]:
                ws.formula_attributes[old_coor]['ref'] = CELL_RE.sub(
                    replace,
                    ws.formula_attributes[old_coor]['ref']
                )

        # Do the magic to set up our actual shift
        if c.row > row_idx:
            old_coor = c.coordinate
            old_cells.add((c.row,c.col_idx))
            c.row += cnt
            new_cells[(c.row,c.col_idx)] = c
            if old_coor in ws.formula_attributes:
                old_fas.add(old_coor)
                fa = ws.formula_attributes[old_coor].copy()
                new_fas[c.coordinate] = fa

    for coor in old_cells:
        del ws._cells[coor]
    ws._cells.update(new_cells)

    for fa in old_fas:
        del ws.formula_attributes[fa]
    ws.formula_attributes.update(new_fas)

    # Next, we need to shift all the Row Dimensions below our new rows down by cnt...
    for row in range(len(ws.row_dimensions)-1+cnt,row_idx+cnt,-1):
        new_rd = copy.copy(ws.row_dimensions[row-cnt])
        new_rd.index = row
        ws.row_dimensions[row] = new_rd
        del ws.row_dimensions[row-cnt]

    # Now, create our new rows, with all the pretty cells
    row_idx += 1
    for row in range(row_idx,row_idx+cnt):
        # Create a Row Dimension for our new row
        new_rd = copy.copy(ws.row_dimensions[row-1])
        new_rd.index = row
        ws.row_dimensions[row] = new_rd
        for col in range(1,ws.max_column):
            col = get_column_letter(col)
            cell = ws['%s%d'%(col,row)]
            cell.value = None
            source = ws['%s%d'%(col,row-1)]
            if copy_style:
                cell.number_format = source.number_format
                cell.font      = source.font.copy()
                cell.alignment = source.alignment.copy()
                cell.border    = source.border.copy()
                cell.fill      = source.fill.copy()
            if fill_formulae and source.data_type == Cell.TYPE_FORMULA:
                s_coor = source.coordinate
                if s_coor in ws.formula_attributes and 'ref' not in ws.formula_attributes[s_coor]:
                    fa = ws.formula_attributes[s_coor].copy()
                    ws.formula_attributes[cell.coordinate] = fa
                # print("Copying formula from cell %s%d to %s%d"%(col,row-1,col,row))
                cell.value = re.sub(
                    "(\$?[A-Z]{1,3}\$?)%d"%(row - 1),
                    lambda m: m.group(1) + str(row),
                    source.value
                )
                cell.data_type = Cell.TYPE_FORMULA

    # Check for Merged Cell Ranges that need to be expanded to contain new cells
    for cr_idx, cr in enumerate(ws.merged_cell_ranges):
        ws.merged_cell_ranges[cr_idx] = CELL_RE.sub(
            replace,
            cr
        )


    return ws


def _get_domain_from_url_str(url,trailing_slash=False):
    '''
    Function return domain from url e.g. url = "https://www.thesteelindex.com/en/restricted-area",
                                    domain = "https://www.thesteelindex.com/ [if trailing_slash is True]
                                    domain = "https://www.thesteelindex.com [if trailing_slash is False]
    '''

    parsed_uri = urlparse(url)
    if trailing_slash:
        domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)
    else:
        domain = '{uri.scheme}://{uri.netloc}'.format(uri=parsed_uri)
    return domain

def get_date_str(t=0):
    '''
    Summary: Function will return a string of format YYYY-MM-DD e.g. 2016-01-01
    Author: Jax
    Date created: 14 Jul 2016
    Date last updated: 14 Jul 2016
    Precondition:
        Required: N/A
        Optional:
            t: t is an integer mark by day. It will return today's date + t. t can be negative for previous day.
    Postcondition:
        Required: N/A
        Optional: N/A
    '''
    the_day = date.today() + timedelta(t)
    the_day_str = the_day.strftime("%Y-%m-%d")
    return the_day_str



def get_date_str_frm_date(the_date,separator="-"):
    '''
    Summary: Function will return a string of format YYYY-MM-DD e.g. 2016-01-01 give a date/datetime object
    Author: Jax
    Date created: 18 Jul 2016
    Date last updated: 18 Jul 2016
    Precondition:
        Required:
            the_date: the date/datetime object
        Optional:
            separator: the separator e.g. "_" or "-". default is "-"
    Postcondition:
        Required: N/A
        Optional: N/A
    '''
    return the_date.strftime("%Y{0}%m{0}%d".format(separator))


def get_date(t=0,the_date=date.today()):
    '''
    Summary: Function will return the a date object
    Author: Jax
    Date created: 15 Jul 2016
    Date last updated: 15 Jul 2016
    Precondition:
        Required: N/A
        Optional:
            t: t is an integer mark by day. It will return today's date + t. t can be negative for previous day.
    Postcondition:
        Required: N/A
        Optional: N/A
    '''
    the_day = the_date + timedelta(t)
    return the_day



def is_bizday(t=date.today()):
    '''
    Summary: Function takes a take and return 'True' for working day, and 'False' for non-working day.
    Author: Jax
    Date created: 18 Jul 2016
    Date last updated: 18 Jul 2016
    Precondition:
        Required: t
    Postcondition:
        return a date object
    '''
    # print t
    if t.weekday() == 6 or t.weekday() == 5:
        return False
    else:
        return True


def get_biz_date(t=0,the_date=date.today()):
    '''
    Summary: Function will return the a business day (i.e. not sat/sun).
    If the t makes the day a non-business day,
    it will keep subtracting till a business day is reached when t is negative
    If t is positive, it will add till a business day is reached

    Author: Jax
    Date created: 15 Jul 2016
    Date last updated: 15 Jul 2016
    Precondition:
        Required: N/A
        Optional:
            t: t is an integer mark by day. It will return today's date + t. t can be negative for previous day.
            the_date: the reference date
    Postcondition:
        return the business day with respect to the_date.
    '''
    the_day = the_date + timedelta(t)
    if(is_bizday(the_day)): #if it is the business day we just return the day
        return the_day
    else: #else not working day...
        if(t>=0):
            return get_biz_date(t+1,the_date)
        else:
            return get_biz_date(t-1,the_date)



def _update_date_col_in_ws(row_to_add_row_id,ws,date_col_id="A",the_date=get_biz_date(-1)):
    '''
    Summary: Function will update a sheet date column row where date is the same as the_date
    Author: Jax
    Date created: 22 Jul 2016
    Date last updated: 22 Jul 2016
    Precondition:
        Required:
            row_to_add_row_id: the row to add the date
            ws: the worksheet of openpyxl
        Optional:
            date_col_id: the date column in string e.g. "A"
            the
    Postcondition:
        the ws will have the date updated
    '''
    date_cell = "{}{}".format(date_col_id,str(row_to_add_row_id))
    ws[date_cell]=the_date
    ws[date_cell].number_format = ws['{}{}'.format(date_col_id,str(row_to_add_row_id-1))].number_format


def xls_conversion_vbscript(src,dest,tmp_path="C:\\tmp",delete_vb=False):
    '''
    Summary: Function eventually converts a xlsb,xlsx,xlsm to a xlsm file. It utilizes a VBScript as in between.
             It returns the path in string of the converted file (local) which is essentially 'dest'.
    Author: Jax
    Date created: 19 Jul 2016
    Date last updated: 19 Jul 2016
    Precondition:
        Required:
            src:
                Src of the xlsb/xlsx file:. e.g. \\sgsing022m\sing8data\Metals Risk Team\Metals_risk_data\PriceStore\PriceStore.xlsb
            dest:
                destination of the xlsm file: e.g. 'C:/Users/j256377/Desktop/Metals/AutoIntern/tmp_data/paper_price/PriceStore.xlsm'
    Postcondition:
        return path of the new xlsm file stored locally
    '''

    src=src.replace('/','\\')
    dest=dest.replace('/','\\')


    #create the directory for the destination and tmp
    directory = os.path.dirname(dest)
    if not os.path.exists(directory):
        os.makedirs(directory)

    if not os.path.exists(tmp_path):
        os.makedirs(tmp_path)


    vbscript = """
        dim filesys
        Set filesys = CreateObject("Scripting.FileSystemObject")

        set objExcel = createObject("Excel.Application")
        objExcel.visible = False
        objExcel.EnableEvents = False

        set objWb = objExcel.Workbooks.Open("{}")

        If filesys.FileExists("{}")  Then
          filesys.DeleteFile("{}")
        End If

        objWb.saveas "{}",52
        objWb.close

    """.format(src,dest,dest,dest)

    convertor_full_path = "{}/xls_convertor_tmp.vbs".format(tmp_path)

    with open(convertor_full_path, "w") as vbscript_file:
        vbscript_file.write(vbscript)

    sp.call("cmd /c {}".format(convertor_full_path)) #this thread will stop untill subprocess is completed

    if delete_vb:
        os.remove(convertor_full_path)

    return dest

def _find_row_to_add_by_date(ws,date_col_idx=0,the_date=get_biz_date(-1),previous_row_t_diff_day=-1):
    '''
    Summary: Function aim is to find the row number to insert the new date of the_date.
             It will return the row id.
             Go through each row looking at the date_col.
             Once the row with the date == to the previous biz day of the_date,
                a) A new row will be added below this row (if the next row of the_date is not found)
                b) the row id return will be its row_id + 1.
    Author: Jax
    Date created: 19 Jul 2016
    Date last updated: 21 Jul 2016
    Precondition:
        Required:
        Optional:
            ws:
                worksheet
            date_col_idx:
                the col index with the date. start with 0
            the_date:
                the date to insert new data. E.g. if date to insert is 19 Jul 2016(Tue), it will look for 18 Jul 2016 (Mon).
            previous_row_t_diff_day:
                the date of the previous row difference in terms of day with the date. e.g. if previous_row_t_diff_day=-7, the_date is 22 Jul, the previous row to look for is 17 Jul
    Postcondition:
        return {"ws", "row_to_add_row_id"}
            ws: the inserted row ws
            row_to_add_row_id: the row_id to add. e.g. row id begins with 1. It is the excel row id.
    '''
    row_to_add_row_id = None
    row_found = False
    row_the_date_found = False

    yesterday = get_biz_date(t=previous_row_t_diff_day,the_date=the_date)

    for row in ws.rows:
        if not row_found:
            if isinstance(row[date_col_idx].value,datetime) and row[date_col_idx].value.date() == yesterday:
                row_to_add_row_id = row[date_col_idx].row + 1
            if row_to_add_row_id: #this is set when the yesterday of the_date is found...
                if isinstance(row[date_col_idx].value,datetime) and row[date_col_idx].value.date() == the_date:
                    row_the_date_found = True
                    break

    if not row_to_add_row_id:
        raise ValueError("[_find_row_to_add_by_date] cannot find the row to insert date {}".format(get_date_str_frm_date(the_date)))

    if not row_the_date_found:
        #insert rows below (which will copy previous row as well...)
        insert_rows(self=ws,row_idx=row_to_add_row_id-1,cnt=1)

    return {"ws":ws,"row_to_add_row_id":row_to_add_row_id}

def _copy(src,dest):
    '''
    This copy function will create directory at the same time for the destination
    '''
    directory = os.path.dirname(dest)
    if not os.path.exists(directory):
        os.makedirs(directory)
    shutil.copy(src ,dest)


def _equal_chinese_char(str1,str2):
    '''
    str1 is the original UTF text typed in the python file
    str2 can be any encoding typically from beautiful soup.
    '''
    str1 = str1.strip()
    str2 = str2.strip()

    equal1 = False
    equal2 = False
    equal3 = False
    equal4 = False
    equal5 = False
    try:

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            equal1 = (str1 == str2.encode('utf-8'))
    except Exception as e:
        pass

    try:
        equal2 = str1.decode("utf-8") == str2
    except Exception as e:
        pass

    try:
        equal3 = unicode(str1.decode("utf-8").encode("gb2312"), encoding='unicode_escape') == str2
    except Exception as e:
        pass

    try:
        equal4 = unicode(str1.encode("gb2312"), encoding='unicode_escape') == str2
    except Exception as e:
        pass

    try:
        equal5 = str1.encode("utf-8") == str2.encode('utf-8')
    except Exception as e:
        pass


    success = (
        equal1
        or equal2
        or equal3
        or equal4
        or equal5
    )
    #
    # if not success:
    #

    return success



def _in_chinese_char(str1,str2):
    '''
    str1 is the original UTF text typed in the python file
    str2 can be any encoding typically from beautiful soup.
    '''
    str1 = str1.strip()
    str2 = str2.strip()

    equal1 = False
    equal2 = False
    equal3 = False
    equal4 = False
    equal5 = False
    try:
        equal1 = (str1 in str2.encode('utf-8'))
    except Exception as e:
        pass

    try:
        equal2 = str1.decode("utf-8") in str2
    except Exception as e:
        pass

    try:
        equal3 = unicode(str1.decode("utf-8").encode("gb2312"), encoding='unicode_escape') in str2
    except Exception as e:
        pass

    try:
        equal4 = unicode(str1.encode("gb2312"), encoding='unicode_escape') in str2
    except Exception as e:
        pass

    try:
        equal5 = str1.encode("utf-8") in str2.encode('utf-8')
    except Exception as e:
        pass

    success = (
        equal1
        or equal2
        or equal3
        or equal4
        or equal5
    )
    #
    # if not success:
    #

    return success



def _get_mysteel_price(
            url = "http://list1.mysteel.com/market/p-228-15278-----0--------1.html", #page 2 is just http://list1.mysteel.com/article/p-3571------0-0-0-----1.html
            the_date=get_biz_date(-1),
            session_requests=None,
            mode = "construction",
            price_name ="上海市场建筑钢材价格行情",
            spec="Φ16-22",
            origin="萍钢",
            product="螺纹钢",
            area="上海",
            suppress_exception = False #set to True if you don't want this function to raise exeception
    ):
    '''
    Get price from mysteel
    Author: Jax
    Date created: 19 Jul 2016
    Date last updated: 21 Jul 2016
    Precondition:
        Required:
        Optional:
            url:
                url of the mysteel first page. default http://list1.mysteel.com/article/p-3571------0-0-0-----1.html
            the_date:
                date of getting the mysteel price. Default the_date=get_biz_date(-1),
            mode:
                mode can be either:
                "furnace_raw": get the 出厂 price blast furnance raw material 炉料 http://list1.mysteel.com/article/p-3571-------------1.html
                "construction": construction material 建材 e.g url = "http://list1.mysteel.com/market/p-228-----010101-0-0104-------1.html"
                    in this mode, additional parameters needed as below
                        "price_name" in chinese is needed for navigating to the page with the price. e.g. "天津市场建筑钢材价格行情" is needed to search for the title
                        "product" in chinese e.g. "螺纹钢"
                        "spec" 规格（mm）e.g.
                        "origin" 钢厂/产地" e.g. "东华钢铁"
                "hrc"/"crc": HRC 热轧 first page e.g. "http://list1.mysteel.com/price/p-10058--010103--1.html"
                "crc": CRC 冷轧 first page e.g. "http://list1.mysteel.com/price/p-10053--010104--1.html"
                    area: e.g. "上海" col
                    spec: e.g. "1.0mm" row
    Postcondition:
        return the price (float). return None if data not found or not ready. Will not raise error if missing data. Just return none.
    '''
    logger = logging.getLogger(__name__)
    if(not session_requests):
        session_requests = _get_login_session(type="mysteel")

    try:
        the_link=None
        max_pg_to_find = 10
        current_pg = 1
        while(not the_link and current_pg < max_pg_to_find):
            url = _change_my_steel_page_html(url,current_pg)
            #fetch the index page
            result = _retry_request(url=url,encoding="gb2312",session_requests=session_requests)
            html_data = result.content
            soup = BeautifulSoup(html_data,"lxml")
            links = soup.find("ul",{"class":"nlist"}).find_all("li",class_=lambda x: x != 'dashed')
            for link in links:
                link_date = datetime.strptime(link.find("span",{"class":"date"}).text,"%Y-%m-%d %H:%M").date()
                if(
                    (not price_name)
                    or
                    #(price_name in link.text)
                    
                    # price_name.encode('utf-8') in link.text.encode('utf-8')
                    #or
                    price_name.decode("gbk") in link.text
                    # or
                    #unicode(price_name.decode("utf-8").encode("gb2312"), encoding='unicode_escape') in link.text.strip()
                     ):
                    if link_date == the_date:
                        logger.info("[_get_mysteel_price] Found the link in page {} for {}. ".format(current_pg,url))
                        the_link = link.find("a").get("href")
                        break #found the link!
                    elif link_date < the_date: #if the page doesn't have the latest figures, stop searching
                        logger.warn("[_get_mysteel_price] missing price {} for this date {}. The max date in the index page is {}".format(url,the_date,link_date))
                        return None #data not found
                else:
                    #print link.text
                    pass
            current_pg = current_pg+1
            if not the_link:
                logger.info("[_get_mysteel_price] Couldn't find in page {} for {}. Trying the page {}".format(current_pg-1,url,current_pg))
        if not the_link:
            print link
            logger.warn("[_get_mysteel_price]14:15 2016/11/2314:15 2016/11/23 couldn't find link in {} for the date {}".format(url,the_date))
            return None

        #a) Furnace raw
        if mode == "furnace_raw":
            #go to the link and find 出厂 price
            result = _retry_request(url=the_link,encoding="gb2312",session_requests=session_requests)
            html_data = result.content
            soup = BeautifulSoup(html_data,"lxml")
            rows = soup.find("table",{"id":"marketTable"}).findAll("tr")
            price = None
            num_row_to_skip = 2
            for idx,row in enumerate(rows):
                if idx >= num_row_to_skip:
                    #print row.find_all("td")[0].text,row.find_all("td")[1].text,row.find_all("td")[3].text.encode("gbk")
                    #print idx,row
                    row_product = row.find_all("td")[0].text.strip()
                    row_spec = row.find_all("td")[1].text.strip()
                    row_origin = row.find_all("td")[3].text.strip()
                    
                    if(
                        row_product == product.decode("gbk")
                        and row_spec == spec.decode("gbk")
                        and  row_origin ==origin.decode("gbk")
                        #row_origin == origin.decode("gbk")
                        #_equal_chinese_char(product,row_product)
                        #and _equal_chinese_char(spec,row_spec)
                        #and _equal_chinese_char(origin,row_origin)
                    ):
                        try:
                            price = float(row.find_all("td")[4].text.strip())
                            break
                        except ValueError:
                            price = 0

            if price is None:
                #dump_path = "_get_mysteel_price_{}".format(datetime.now().strftime("%Y-%m-%d_%H_%M_%S"))
                logger.warn("[_get_mysteel_price] missing price {} for this date {}. Is the login ID still working? Check the dump at {}".format(url,the_date,dump_path))
                #with open(dump_path, "w") as text_file:
                #    text_file.write("{}.txt".format(html_data))
                #with codecs.open("{}_compared.txt".format(dump_path), "w", encoding="utf-8") as text_file:
                #    text_file.write("product:" + product.decode('utf-8') + ";spec:" +spec.decode('utf-8')+" ;origin:"+origin.decode('utf-8'))
                #with codecs.open(dump_path, "w", "gb2312") as text_file:
                #    text_file.write(html_data.encode('gb2312'))
            return price
            
        if mode == "js_furnace_raw":
            #go to the link and find 出厂 price
            result = _retry_request(url=the_link,encoding="gb2312",session_requests=session_requests)
            html_data = result.content
            soup = BeautifulSoup(html_data,"lxml")
            rows = soup.find("table",{"id":"marketTable"}).findAll("tr")
            price = None
            num_row_to_skip = 1
            for idx,row in enumerate(rows):
                if idx >= num_row_to_skip:
                    #print row.find_all("td")[0].text,row.find_all("td")[1].text,row.find_all("td")[3].text.encode("gbk")
                    #print idx,row
                    row_product = row.find_all("td")[1].text.strip()
                    row_spec = row.find_all("td")[2].text.strip()
                    row_origin = row.find_all("td")[3].text.strip()
                    if(
                        row_product == product.decode("gbk")
                        and row_spec == spec.decode("gbk")
                        and  row_origin ==origin.decode("gbk")
                        #row_origin == origin.decode("gbk")

                        #_equal_chinese_char(product,row_product)
                        #and _equal_chinese_char(spec,row_spec)
                        #and _equal_chinese_char(origin,row_origin)
                    ):
                        try:
                            price = (row.find_all("td")[4].text.strip())
                            break
                        except ValueError:
                            price = 0
            '''
            if isinstance(price, (str, unicode)):
                price =0
            '''
            
            if price is None:
                #dump_path = "_get_mysteel_price_{}".format(datetime.now().strftime("%Y-%m-%d_%H_%M_%S"))
                logger.warn("[_get_mysteel_price] missing price {} for this date {}. Is the login ID still working? Check the dump at {}".format(url,the_date,dump_path))
                #with open(dump_path, "w") as text_file:
                #    text_file.write("{}.txt".format(html_data))
                #with codecs.open("{}_compared.txt".format(dump_path), "w", encoding="utf-8") as text_file:
                #    text_file.write("product:" + product.decode('utf-8') + ";spec:" +spec.decode('utf-8')+" ;origin:"+origin.decode('utf-8'))
                #with codecs.open(dump_path, "w", "gb2312") as text_file:
                #    text_file.write(html_data.encode('gb2312'))
            return price

        #b) construction material
        elif mode == "construction":
            #go to the link and find the price,
            result = _retry_request(url=the_link,encoding=None,session_requests=session_requests)
            html_data = result.content
            #html_data = html_data.decode("gb2312","ignore")
            soup = BeautifulSoup(html_data,"lxml")
            #from_encoding="gbk"
            rows = soup.find("table",{"id":"marketTable"}).findAll("tr")
            price = None
            num_row_to_skip = 2
            for idx,row in enumerate(rows):
                if idx >= num_row_to_skip:
                    #print row.find_all("td")[0].text,row.find_all("td")[1].text,row.find_all("td")[3].text.encode("gbk")
                    #print idx,row
                    if product.decode("gbk") == "SPHC":
                        row_product = row.find_all("td")[2].text.strip()
                    else:
                        row_product = row.find_all("td")[0].text.strip()
                    row_spec = row.find_all("td")[1].text.strip()
                    row_origin = row.find_all("td")[3].text.strip()
                    if(
                        row_product == product.decode("gbk")
                        and row_spec == spec.decode("gbk")
                        and  row_origin ==origin.decode("gbk")
                        #row_origin == origin.decode("gbk")

                        #_equal_chinese_char(product,row_product)
                        #and _equal_chinese_char(spec,row_spec)
                        #and _equal_chinese_char(origin,row_origin)
                    ):
                        try:
                            price = float(row.find_all("td")[4].text.strip())
                            break
                        except ValueError:
                            price = 0

            if price is None:
                #dump_path = "_get_mysteel_price_{}".format(datetime.now().strftime("%Y-%m-%d_%H_%M_%S"))
                logger.warn("[_get_mysteel_price] missing price {} for this date {}. Is the login ID still working? Check the dump at {}".format(url,the_date,dump_path))
                #with open(dump_path, "w") as text_file:
                #    text_file.write("{}.txt".format(html_data))
                #with codecs.open("{}_compared.txt".format(dump_path), "w", encoding="utf-8") as text_file:
                #    text_file.write("product:" + product.decode('utf-8') + ";spec:" +spec.decode('utf-8')+" ;origin:"+origin.decode('utf-8'))
                #with codecs.open(dump_path, "w", "gb2312") as text_file:
                #    text_file.write(html_data.encode('gb2312'))
            return price

        elif mode == "crc" or mode == "hrc":
            #go to the link
            result = _retry_request(url=the_link,encoding="gb2312",session_requests=session_requests)
            html_data = result.content
            soup = BeautifulSoup(html_data,"lxml")
            #print soup
            rows = soup.find("table",{"id":"priceTable"}).find_all("tr")
            price = None
            col_idx = None
            row_idx = None
            for idx,row in enumerate(rows):
                if idx == 0: #for first row...
                    for cidx,td in enumerate(row.find_all("td")):
                        # if td.text.strip() == area.decode("utf-8"):
                        if td.text==area.decode("gbk"):
                            col_idx = cidx
                            break
                else:
                    if(
                        # row.find_all("td")[0].text.strip() == spec.decode("utf-8")
                        row.find_all("td")[0].text==spec.decode("gbk")
                        #_equal_chinese_char(spec,row.find_all("td")[0].text)
                    ):
                        row_idx = idx
                        break

            if col_idx and row_idx:
                price = float(rows[row_idx].find_all("td")[col_idx].text.strip())
            else:
                logger.warn("[_get_mysteel_price] missing price {} for this date {}. Is the login ID still working?".format(url,the_date))
            return price

        elif mode == "coal":
            row_label = "价格"
            #go to the link
            result = _retry_request(url=the_link,encoding="gb2312",session_requests=session_requests)
            html_data = result.content
            soup = BeautifulSoup(html_data,"lxml")
            #print soup
            rows = soup.find("table").find_all("tr")
            price = None
            col_idx = None
            row_idx = 2
            for idx,row in enumerate(rows):
                if idx == 0: #for first row...
                    for cidx,td in enumerate(row.find_all("td")):
                        # if td.text.strip() == area.decode("utf-8"):
                        if td.text==area.decode("gbk"):
                            col_idx = cidx
                            break
                else:
                    if row.find_all("td")[0].text.strip() == row_label.decode("gbk"):
                        row_idx = idx
                        break
            if col_idx:
                price = float(rows[row_idx].find_all("td")[col_idx].text.strip())
            else:
                logger.warn("[_get_mysteel_price] missing price {} for this date {}. Is the login ID still working?".format(url,the_date))
            return price

        else:
            raise ValueError("Invalid mode for _get_mysteel_price! Your mode is {}".format(mode))
    except Exception as e:
        msg = "[_get_mysteel_price] Error getting data from my steel. Is the login still working or the page has changed? Error msg: {}".format(str(e))
        if suppress_exception:
            logger.warning(msg)
        else:
            logger.error(msg)
            raise

    print price


def _get_sina_finance_future_price(
        url='http://vip.stock.finance.sina.com.cn/q/view/vFutures_History.php?jys=shfe&pz=RB&hy=RB0&breed=RB0&type=inner',
        suppress_exception=False
    ):
    '''
    Summary: Function pull closing price from finance.sina.com.cn
    Author: Jax
    Date created: 19 Jul 2016
    Date last updated: 19 Jul 2016
    Precondition:
        Required:
        Optional:
            the_date:
                the date of the data to get the numbers
            url:
                url of the finance.sina.com.cn e.g.http://vip.stock.finance.sina.com.cn/q/view/vFutures_History.php?jys=shfe&pz=RB&hy=RB0&breed=RB0&type=inner
    Postcondition:
        return the price in float
    '''
    logger = logging.getLogger(__name__)

    _the_date=date.today()-timedelta(1)
    date_str = get_date_str_frm_date(_the_date)
    url_with_date = "{}&start={}&end={}".format(url,date_str,date_str)

    # logger.debug("[_get_sina_finance_future_price] Fetching future price from {}...".format(url_with_date))
    # r  = requests.get(url_with_date,headers=headers)
    # logger.debug("[_get_sina_finance_future_price] Fetched future price from {}!".format(url_with_date))

    r = _retry_request(url=url_with_date)

    data = r.text
    soup = BeautifulSoup(data,"lxml")
    
    tr_history = soup.find("tr", {"class" : "tr_2"})

    x=float(tr_history.find_all('div')[1].contents[0])
    
    return x
    '''
    except Exception as e:
        err_msg = "[_get_sina_finance_future_price] Error scraping from sina finance. Error msg: {}".format(str(e))
        if not suppress_exception:
            logger.error(err_msg)
            raise
        else:
            logger.warn(err_msg)
   '''
    print x

def _retry_request(
        url,
        session_requests = None,
        mode = "GET",
        num_tries = 10,
        delay = 3,
        encoding = 'utf-8',
        debug = False,
        timeout = 300
    ):
    '''
    Summary: Function will keep retrying for num_tries time sleeping for 5 seconds in between each try
    Author: Jax
    Date created: 28 Jul 2016
    Date last updated: 28 Jul 2016
    Precondition:
        Required:
            url:
                url that you want to get/post
        Optional:
            mode:
                mode of the request can be "GET" or "POST"
            delay:
                the number of seconds to delay before each retry
            encoding:
                The encoding type of the doc. Default is UTF8. For mysteel it is "gb2312"
    Postcondition:
        return results of the request if didn't fail...
    '''
    logger = logging.getLogger(__name__)
    time.sleep(0.5)
    success = False
    i = 0
    while not success or i < num_tries:
        try:
            the_request = None
            if not session_requests:
            #a)Non session mode
                the_request = requests
            else:
            #b) Session mode
                the_request = session_requests



            result = None
            if mode == "POST":
                if debug:
                    print("[_retry_request] fetching [POST] {}".format(url))
                logger.debug("[_retry_request] fetching [POST] {}".format(url))
                result = the_request.post(url,headers=headers,timeout=timeout,proxies=proxyDict)
                result.encoding = encoding
                logger.debug("[_retry_request] fetched [POST] {}".format(url))
                if debug:
                    print("[_retry_request] fetched [POST] {}".format(url))
            else:
                if debug:
                    print("[_retry_request] fetching [GET] {}".format(url))
                logger.debug("[_retry_request] fetching [GET] {}".format(url))
                result = the_request.get(url,headers=headers,timeout=timeout,proxies=proxyDict)
                result.encoding = encoding
                logger.debug("[_retry_request] fetched [GET] {}".format(url))
                if debug:
                    print("[_retry_request] fetched [GET] {}".format(url))

            #Check to make sure the content is not empty
            if result.content.strip() == "":
                raise ValueError("[_retry_request] Empty results")
            elif "Bad Gateway" in result.content.strip():
                #dump_path = "_retry_request_{}.txt".format(datetime.now().strftime("%Y-%m-%d_%H_%M_%S"))
                logger.warn("[_retry_request] Cargill Proxy error DUMP: {}".format())
                #with open(dump_path, "w") as text_file:
                #    text_file.write("{}".format(result.content))
            else:
                return result

        except Exception as e:
            i = i + 1
            # print "_retry_request failed! Retrying for {}th time".format(str(i))
            logger.debug("[_retry_request] _retry_request failed! Retrying for {}th time. Error msg: {}".format(str(i),str(e)))
            if debug:
                print("[_retry_request] _retry_request failed! Retrying for {}th time".format(str(i)))
            time.sleep(delay)
            continue
        break

    #Retry still fails!
    raise ValueError("[_retry_request] _retry_request still failed after retrying for {} times".format(str(num_tries)))

def _get_login_session(type="steelindex",debug=False):
    '''
    Summary: Function logge into website and return the session_requests
    Author: Jax
    Date created: 27 Jul 2016
    Date last updated: 27 Jul 2016
    Precondition:
        Required:
            "type": The type of logged in session to return. default is steelindex
            It can be one of the below:
                1) steelindex
                2) steelbb
        Optional:
    Postcondition:
        return session_requests object with login
    '''
    logger = logging.getLogger(__name__)

    delay = 5
    num_tries = 5
    payload = _session_data[type]
    session_requests = requests.session()

    if debug:
        ("[_get_login_session] Login to {}...".format(_session_data[type]["login_url"]))

    logger.debug("[_get_login_session] Login to {}...".format(_session_data[type]["login_url"]))
    success = False
    i = 0
    while not success or i < num_tries:
        try:
            result = session_requests.post(
            	_session_data[type]["login_url"],
            	data = payload,
                allow_redirects=False,
                headers=headers,
                proxies=proxyDict
            )
            success=True
            break
        except Exception as e:
            i = i + 1
            # print "_retry_request failed! Retrying for {}th time".format(str(i))
            logger.debug("_get_login_session failed! Retrying for {}th time. Error msg {}".format(str(i),str(e)))
            if debug:
                print("_get_login_session failed! Retrying for {}th time".format(str(i)))
            time.sleep(delay)
            continue

    logger.debug("[_get_login_session] Logged in to {}!".format(_session_data[type]["login_url"]))
    if debug:
        ("[_get_login_session] Logged in to {}!".format(_session_data[type]["login_url"]))
    return session_requests

def get_mysteel_data(product_list, tday, suppress_exception = False):
    logger = logging.getLogger(__name__)
    try:
        session_requests = _get_login_session(type="mysteel")
        price = {}
        for product in product_list:
            setting = dict( mysteel_url_map[product].items() + {"suppress_exception": suppress_exception, "the_date": tday,
                                                    "session_requests": session_requests}.items())
            price[product] = _get_mysteel_price(**setting)
            logger.info("{} price: {} (www.mysteel.com)".format(product, str(price[product])))
    except Exception as e:
        logger.error("II) Error in updating ChinaPrices get the URL of mysteel")
        raise
    return price

def update_mysteel_xl(tday = date.today(), batch = 'PM', copy_from_src = True,
                    src_xl = '//mscnshan022.ap.corp.cargill.com/data/Ferrous/China Steel Team/China Steel Analysis/AM PM Price/Daily Steel price Update.xlsx',
                    tmp_xl = '//mscnshan022.ap.corp.cargill.com/data/Ferrous/China Steel Team/China Steel Analysis/AM PM Price/price.xlsx',
                    ):
    base.config_logging(__name__ + ".log", level=logging.DEBUG,
                        format='%(name)s:%(funcName)s:%(lineno)d:%(asctime)s %(levelname)s %(message)s',
                        to_console=True,
                        console_level=logging.INFO)
    logger = logging.getLogger(__name__)
    logger.info("Processing for today {}".format(tday.strftime("%Y-%m-%d (%a)")))
    product_list = []
    xl_loc = {}
    if batch == 'PM':
        excel_tab = batch + " prices"
    else:
        excel_tab = batch + " Prices"
    for product in mysteel_excel_columns:
        if batch in mysteel_excel_columns[product]:
            product_list.append(product)
            xl_loc[product] = mysteel_excel_columns[product][batch]

    price = get_mysteel_data(product_list, tday, suppress_exception = False)

    #Copy Prices.xlsx to local.
    if copy_from_src:
        _copy(src_xl, tmp_xl)

    #load the excel file for writing
    wb = openpyxl.load_workbook(tmp_xl)
    ws = wb[excel_tab]
    result = _find_row_to_add_by_date(ws = ws, date_col_idx = 0, the_date = tday)
    row_to_add_row_id = result['row_to_add_row_id']
    _update_date_col_in_ws(row_to_add_row_id,ws, date_col_id="A", the_date = tday)
    for product in product_list:
        cell = '{}{}'.format(xl_loc[product], str(row_to_add_row_id))
        ws[cell] = price[product]
        ws[cell].number_format = ws['{}{}'.format(xl_loc[product], str(row_to_add_row_id - 1))].number_format
    wb.save(tmp_xl)
